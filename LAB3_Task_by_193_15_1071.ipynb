{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrPRS23SpS6Iwteydgqksq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaminMahdi/All-.ipynb/blob/master/LAB3_Task_by_193_15_1071.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF_4noFZw3EE",
        "outputId": "69ac7edf-eb8a-41ca-ba7b-e699f9121f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.39\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "--2022-09-15 15:02:27--  https://mirrors.estointernet.in/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving mirrors.estointernet.in (mirrors.estointernet.in)... 43.255.166.254, 2403:8940:3:1::f\n",
            "Connecting to mirrors.estointernet.in (mirrors.estointernet.in)|43.255.166.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/octet-stream]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop3.2.tgz.4’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M  12.3MB/s    in 20s     \n",
            "\n",
            "2022-09-15 15:02:48 (10.8 MB/s) - ‘spark-3.1.2-bin-hadoop3.2.tgz.4’ saved [228834641/228834641]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q gdown && gdown --fuzzy https://drive.google.com/file/d/1HFIClmLVILw851oU3n-hAz3zXkcMZK-I/view      \n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2_by_yk\""
      ],
      "metadata": {
        "id": "EC9-dwhvxZlw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"RDD\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "BnxQIn3WxcD5",
        "outputId": "601cf3ee-4b45-4f42-edd2-c8090478d15d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fc18d084dbf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[1]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RDD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    161\u001b[0m             raise Exception(\n\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 )\n\u001b[1;32m    165\u001b[0m             )\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.1.2-bin-hadoop3.2_by_yk/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.textFile(\"/content/yk.txt\")\n",
        "rdd.collect()"
      ],
      "metadata": {
        "id": "uaJqEG9cxml_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
        "rdd2.take(20)"
      ],
      "metadata": {
        "id": "69EcumhAyKoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3 = rdd2.map(lambda x: (x,1))\n",
        "rdd3.take(10)"
      ],
      "metadata": {
        "id": "l3raYrqzysvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd4 = rdd3.reduceByKey(lambda a,b:a+b)\n",
        "rdd4.take(10)"
      ],
      "metadata": {
        "id": "XuEXtQwtzCFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd5 = rdd4.map(lambda x: (x[0], x[1])).sortByKey()\n",
        "rdd5.take(10)"
      ],
      "metadata": {
        "id": "N_4-8Mnk0tPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#id: 193-15-1071\n",
        "rdd6 = rdd5.repartition(7)  #using 1 doesn't make any sense\n",
        "print(\"Number of Partitions: \"+ str(rdd6.getNumPartitions()))"
      ],
      "metadata": {
        "id": "zOKEQddgEnsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd6.saveAsTextFile(\"/content/export\")"
      ],
      "metadata": {
        "id": "jVyMapoC34Q1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}